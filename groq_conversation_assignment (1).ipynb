{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai -q\n"
      ],
      "metadata": {
        "id": "951RfBA3m5ZE"
      },
      "id": "951RfBA3m5ZE",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# âœ… Set Groq API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# âœ… Use Groq's OpenAI-compatible endpoint\n",
        "client = OpenAI(\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    base_url=\"https://api.groq.com/openai/v1\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Conversation Memory with Summarization\n",
        "# -----------------------------\n",
        "class ConversationMemory:\n",
        "    def __init__(self, max_messages=5, model=\"llama-3.1-8b-instant\"):\n",
        "        self.history = []\n",
        "        self.max_messages = max_messages\n",
        "        self.model = model\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "        if len(self.history) > self.max_messages:\n",
        "            self.summarize_history()\n",
        "\n",
        "    def summarize_history(self):\n",
        "        response = client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=self.history,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        summary = response.choices[0].message.content\n",
        "        self.history = [\n",
        "            {\"role\": \"system\", \"content\": f\"Summary: {summary}\"}\n",
        "        ]\n",
        "\n",
        "# -----------------------------\n",
        "# Extraction Function (Structured JSON)\n",
        "# -----------------------------\n",
        "def extract_info_from_chat(chat_history):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        response_format={\"type\": \"json_object\"},  # âœ… FIXED\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Extract the user's name and topic from this chat. Output JSON with keys 'name' and 'topic'.\"},\n",
        "            {\"role\": \"user\", \"content\": str(chat_history)}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# -----------------------------\n",
        "# Demo Usage\n",
        "# -----------------------------\n",
        "cm = ConversationMemory(max_messages=3)\n",
        "\n",
        "# Add some messages\n",
        "cm.add_message('user', 'Hello there!')\n",
        "cm.add_message('assistant', 'Hi! How can I help?')\n",
        "cm.add_message('user', 'Can you tell me a joke?')\n",
        "\n",
        "print(\"After 3 messages (summarized):\\n\", cm.history, \"\\n\")\n",
        "\n",
        "cm.add_message('assistant', 'Why did the chicken cross the road? To get to the other side!')\n",
        "cm.add_message('user', 'Haha, good one!')\n",
        "\n",
        "print(\"After 5 messages (with truncation):\\n\", cm.history, \"\\n\")\n",
        "\n",
        "# Sample chats for extraction\n",
        "samples = [\n",
        "    [\n",
        "        {\"role\":\"user\",\"content\":\"Hi, I am Alex.\"},\n",
        "        {\"role\":\"assistant\",\"content\":\"Nice to meet you, Alex. What are we discussing?\"},\n",
        "        {\"role\":\"user\",\"content\":\"I want to talk about machine learning.\"}\n",
        "    ],\n",
        "    [\n",
        "        {\"role\":\"user\",\"content\":\"Hey, I am Priya.\"},\n",
        "        {\"role\":\"assistant\",\"content\":\"Hello Priya, what would you like to talk about?\"},\n",
        "        {\"role\":\"user\",\"content\":\"Let's talk about space travel.\"}\n",
        "    ]\n",
        "]\n",
        "\n",
        "for i, chat in enumerate(samples, start=1):\n",
        "    print(f\"\\n--- Sample Chat {i} ---\")\n",
        "    extracted = extract_info_from_chat(chat)\n",
        "    print(\"Raw Extracted JSON:\\n\", extracted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSpNY8jenFEq",
        "outputId": "233df33f-e9d7-419f-abbe-d2422b857270"
      },
      "id": "VSpNY8jenFEq",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After 3 messages (summarized):\n",
            " [{'role': 'user', 'content': 'Hello there!'}, {'role': 'assistant', 'content': 'Hi! How can I help?'}, {'role': 'user', 'content': 'Can you tell me a joke?'}] \n",
            "\n",
            "After 5 messages (with truncation):\n",
            " [{'role': 'system', 'content': \"Summary:  (Classic, I know, but it's a good one.)\\n\\nWant to hear another one?\"}, {'role': 'user', 'content': 'Haha, good one!'}] \n",
            "\n",
            "\n",
            "--- Sample Chat 1 ---\n",
            "Raw Extracted JSON:\n",
            " {\n",
            "   \"name\":\"Alex\",\n",
            "   \"topic\":\"machine learning\"\n",
            "}\n",
            "\n",
            "--- Sample Chat 2 ---\n",
            "Raw Extracted JSON:\n",
            " {\n",
            "   \"name\": \"Priya\",\n",
            "   \"topic\": \"space travel\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConversationMemory(max_messages=5)  # you can change max_messages if you want\n",
        "\n",
        "print(\"Start chatting! Type 'exit' to stop.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Chat ended.\")\n",
        "        break\n",
        "\n",
        "    # add user message to history\n",
        "    cm.add_message(\"user\", user_input)\n",
        "\n",
        "    # get assistant reply from Groq\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=cm.history\n",
        "    )\n",
        "\n",
        "    assistant_reply = response.choices[0].message.content\n",
        "    print(\"Assistant:\", assistant_reply)\n",
        "\n",
        "    # add assistant reply to history\n",
        "    cm.add_message(\"assistant\", assistant_reply)\n",
        "\n",
        "    # Show current history (optional, for debugging)\n",
        "    print(\"\\nðŸ“œ Current Conversation History:\", cm.history, \"\\n\")\n"
      ],
      "metadata": {
        "id": "IVBvbSt0n6pV"
      },
      "id": "IVBvbSt0n6pV",
      "execution_count": null,
      "outputs": []
    }
  ]
}